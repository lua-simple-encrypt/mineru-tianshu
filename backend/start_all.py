#!/usr/bin/env python3
"""
MinerU Tianshu - å¯åŠ¨æ‰€æœ‰æœåŠ¡ (All-in-One)

1. VLLM Server (å¯é€‰) - ç«¯å£ 8003 (ç”¨äº PaddleOCR-VL)
2. API Server (FastAPI) - ç«¯å£ 8000
3. LitServe Worker Pool - ç«¯å£ 8001
4. Task Scheduler (å¯é€‰) - åå°ä»»åŠ¡è°ƒåº¦
5. MCP Server (å¯é€‰) - ç«¯å£ 8002

è‡ªåŠ¨æ£€æŸ¥å¹¶ä¸‹è½½ OCR æ¨¡å‹ï¼ˆPaddleOCR-VLï¼‰
æ”¯æŒ GPU åŠ é€Ÿã€ä»»åŠ¡é˜Ÿåˆ—ã€ä¼˜å…ˆçº§ç®¡ç†
"""

import subprocess
import signal
import sys
import time
import os
import requests
from loguru import logger
from pathlib import Path
import argparse
from utils import parse_list_arg
from dotenv import load_dotenv


class TianshuLauncher:
    """å¤©æ¢æœåŠ¡å¯åŠ¨å™¨"""

    def __init__(
        self,
        output_dir="/tmp/mineru_tianshu_output",
        api_port=8000,
        worker_port=8001,
        workers_per_device=1,
        devices="auto",
        accelerator="auto",
        enable_mcp=False,
        mcp_port=8002,
        # PaddleOCR VL VLLM é…ç½®
        paddleocr_vl_vllm_engine_enabled=False,
        paddleocr_vl_vllm_api_list=[],
        # æœ¬åœ° VLLM å¯åŠ¨é…ç½®
        start_local_vllm=False,
        vllm_model_path=None,
        vllm_port=8003,
        vllm_gpu_util=0.4,
        vllm_max_model_len=8192,
    ):
        self.output_dir = output_dir
        self.api_port = api_port
        self.worker_port = worker_port
        self.workers_per_device = workers_per_device
        self.devices = devices
        self.accelerator = accelerator
        self.enable_mcp = enable_mcp
        self.mcp_port = mcp_port
        self.processes = []
        
        # VLLM ç›¸å…³é…ç½®
        self.paddleocr_vl_vllm_engine_enabled = paddleocr_vl_vllm_engine_enabled
        self.paddleocr_vl_vllm_api_list = paddleocr_vl_vllm_api_list
        self.start_local_vllm = start_local_vllm
        self.vllm_model_path = vllm_model_path
        self.vllm_port = vllm_port
        self.vllm_gpu_util = vllm_gpu_util
        self.vllm_max_model_len = vllm_max_model_len

    def check_ocr_models(self):
        """æ£€æŸ¥å¹¶ä¸‹è½½æ‰€æœ‰ OCR æ¨¡å‹ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡å¯åŠ¨ï¼‰"""
        import threading

        # 1. æ£€æŸ¥ PaddleOCR-VL æ¨¡å‹
        def check_paddleocr_vl():
            try:
                from paddleocr_vl import PaddleOCRVLEngine

                logger.info("ğŸ” Checking PaddleOCR-VL...")
                logger.info("   Note: PaddleOCR-VL models are auto-managed by PaddleOCR")
                
                # ç®€å•åˆå§‹åŒ–å¼•æ“ï¼ˆä¸è§¦å‘ä¸‹è½½ï¼‰
                try:
                    PaddleOCRVLEngine()
                    logger.info("âœ… PaddleOCR-VL engine initialized successfully")
                except Exception as e:
                    # å¦‚æœæ˜¯å› ä¸ºç¼ºå°‘ API è¿æ¥å¯¼è‡´çš„é”™è¯¯æ˜¯æ­£å¸¸çš„ï¼Œåªè¦åŒ…åœ¨å°±è¡Œ
                    logger.debug(f"PaddleOCR-VL init check: {e}")

            except ImportError:
                logger.debug("PaddleOCR-VL not installed, skipping check")
            except Exception as e:
                logger.debug(f"PaddleOCR-VL check skipped: {e}")

        # åœ¨åå°çº¿ç¨‹ä¸­ä¸‹è½½/æ£€æŸ¥æ¨¡å‹
        thread_paddleocr = threading.Thread(target=check_paddleocr_vl, daemon=True)
        thread_paddleocr.start()

    def wait_for_vllm(self, port, timeout=300):
        """ç­‰å¾… VLLM æœåŠ¡å¯åŠ¨å°±ç»ª"""
        start_time = time.time()
        health_url = f"http://localhost:{port}/v1/models"
        
        logger.info(f"â³ Waiting for VLLM to load model at {health_url}...")
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(health_url)
                if response.status_code == 200:
                    logger.info("âœ… VLLM Service is ready!")
                    return True
            except requests.RequestException:
                pass
            
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨
            for name, proc in self.processes:
                if name == "VLLM Service" and proc.poll() is not None:
                    logger.error("âŒ VLLM process died while starting!")
                    return False
            
            time.sleep(2)
            
        logger.error("âŒ Timeout waiting for VLLM to start.")
        return False

    def start_services(self):
        """å¯åŠ¨æ‰€æœ‰æœåŠ¡"""
        logger.info("=" * 70)
        logger.info("ğŸš€ MinerU Tianshu - AI Data Preprocessing Platform")
        logger.info("=" * 70)

        try:
            # è®¡ç®—æ€»æœåŠ¡æ•°
            total_services = 3
            if self.enable_mcp: total_services += 1
            if self.start_local_vllm: total_services += 1
            
            current_step = 1

            # ---------------------------------------------------------
            # 0. (å¯é€‰) å¯åŠ¨æœ¬åœ° VLLM æœåŠ¡
            # ---------------------------------------------------------
            if self.start_local_vllm:
                logger.info(f"ğŸ§  [{current_step}/{total_services}] Starting Local VLLM Service...")
                
                if not self.vllm_model_path:
                    logger.error("âŒ --vllm-model-path is required when --start-local-vllm is enabled")
                    return False

                # æ„å»º VLLM å¯åŠ¨å‘½ä»¤
                # ä½¿ç”¨ python -m vllm.entrypoints.openai.api_server ä»¥ç¡®ä¿ä½¿ç”¨å½“å‰ç¯å¢ƒ
                vllm_cmd = [
                    sys.executable, "-m", "vllm.entrypoints.openai.api_server",
                    "--model", self.vllm_model_path,
                    "--port", str(self.vllm_port),
                    "--gpu-memory-utilization", str(self.vllm_gpu_util),
                    "--max-model-len", str(self.vllm_max_model_len),
                    "--trust-remote-code",
                    "--served-model-name", "paddleocr-vl" # å›ºå®šæ¨¡å‹åç§°æ–¹ä¾¿è°ƒç”¨
                ]
                
                # å¦‚æœæŒ‡å®šäº† deviceï¼Œå¯èƒ½éœ€è¦è®¾ç½® CUDA_VISIBLE_DEVICES
                vllm_env = os.environ.copy()
                if self.devices != "auto" and isinstance(self.devices, list):
                     # å‡è®¾ VLLM å ç”¨ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼Œå…¶ä½™ç»™ Workerï¼Œæˆ–è€…ç”¨æˆ·éœ€è‡ªè¡Œé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶
                     # è¿™é‡Œç®€å•å¤„ç†ï¼šè®© VLLM çœ‹æ‰€æœ‰å¡ï¼Œé€šè¿‡ tensor-parallel-size æ§åˆ¶ï¼ˆæœªåœ¨æ­¤å¤„æš´éœ²ï¼‰
                     pass

                vllm_proc = subprocess.Popen(vllm_cmd, env=vllm_env)
                self.processes.append(("VLLM Service", vllm_proc))
                
                # ç­‰å¾… VLLM å°±ç»ª
                if not self.wait_for_vllm(self.vllm_port):
                    return False
                
                # è‡ªåŠ¨å°†æœ¬åœ° VLLM åœ°å€åŠ å…¥åˆ—è¡¨
                local_vllm_url = f"http://localhost:{self.vllm_port}/v1"
                if local_vllm_url not in self.paddleocr_vl_vllm_api_list:
                    self.paddleocr_vl_vllm_api_list.append(local_vllm_url)
                    logger.info(f"ğŸ”— Added local VLLM to API list: {local_vllm_url}")
                
                current_step += 1
                logger.info("")

            # ---------------------------------------------------------
            # 1. å¯åŠ¨ API Server
            # ---------------------------------------------------------
            logger.info(f"ğŸ“¡ [{current_step}/{total_services}] Starting API Server...")
            env = os.environ.copy()
            env["API_PORT"] = str(self.api_port)
            env["OUTPUT_PATH"] = self.output_dir
            api_proc = subprocess.Popen([sys.executable, "api_server.py"], cwd=Path(__file__).parent, env=env)
            self.processes.append(("API Server", api_proc))
            time.sleep(3)

            if api_proc.poll() is not None:
                logger.error("âŒ API Server failed to start!")
                return False

            logger.info(f"   âœ… API Server started (PID: {api_proc.pid})")
            logger.info(f"   ğŸ“– API Docs: http://localhost:{self.api_port}/docs")
            current_step += 1
            logger.info("")

            # ---------------------------------------------------------
            # 2. å¯åŠ¨ LitServe Worker Pool
            # ---------------------------------------------------------
            logger.info(f"âš™ï¸  [{current_step}/{total_services}] Starting LitServe Worker Pool...")
            worker_env = os.environ.copy()
            worker_env["WORKER_PORT"] = str(self.worker_port)
            worker_env["OUTPUT_PATH"] = self.output_dir

            worker_cmd = [
                sys.executable,
                "litserve_worker.py",
                "--output-dir", self.output_dir,
                "--accelerator", self.accelerator,
                "--workers-per-device", str(self.workers_per_device),
                "--port", str(self.worker_port),
                "--devices", str(self.devices) if isinstance(self.devices, str) else ",".join(map(str, self.devices)),
            ]

            # VLLM å‚æ•°é€ä¼ 
            if self.paddleocr_vl_vllm_engine_enabled:
                worker_cmd.extend(["--paddleocr-vl-vllm-engine-enabled"])
            
            # æ­¤æ—¶ self.paddleocr_vl_vllm_api_list å¯èƒ½å·²ç»åŒ…å«æœ¬åœ°å¯åŠ¨çš„ VLLM
            worker_cmd.extend(["--paddleocr-vl-vllm-api-list", str(self.paddleocr_vl_vllm_api_list)])

            worker_proc = subprocess.Popen(worker_cmd, cwd=Path(__file__).parent, env=worker_env)
            self.processes.append(("LitServe Workers", worker_proc))
            time.sleep(5)

            if worker_proc.poll() is not None:
                logger.error("âŒ LitServe Workers failed to start!")
                return False

            logger.info(f"   âœ… LitServe Workers started (PID: {worker_proc.pid})")
            current_step += 1
            logger.info("")

            # ---------------------------------------------------------
            # 3. å¯åŠ¨ Task Scheduler
            # ---------------------------------------------------------
            logger.info(f"ğŸ”„ [{current_step}/{total_services}] Starting Task Scheduler...")
            scheduler_cmd = [
                sys.executable,
                "task_scheduler.py",
                "--litserve-url", f"http://localhost:{self.worker_port}/predict",
                "--wait-for-workers",
            ]

            scheduler_proc = subprocess.Popen(scheduler_cmd, cwd=Path(__file__).parent)
            self.processes.append(("Task Scheduler", scheduler_proc))
            time.sleep(3)

            if scheduler_proc.poll() is not None:
                logger.error("âŒ Task Scheduler failed to start!")
                return False

            logger.info(f"   âœ… Task Scheduler started (PID: {scheduler_proc.pid})")
            current_step += 1
            logger.info("")

            # ---------------------------------------------------------
            # 4. å¯åŠ¨ MCP Serverï¼ˆå¯é€‰ï¼‰
            # ---------------------------------------------------------
            if self.enable_mcp:
                logger.info(f"ğŸ”Œ [{current_step}/{total_services}] Starting MCP Server...")
                mcp_env = os.environ.copy()
                mcp_env["API_BASE_URL"] = f"http://localhost:{self.api_port}"
                mcp_env["MCP_PORT"] = str(self.mcp_port)
                mcp_env["MCP_HOST"] = "0.0.0.0"

                mcp_proc = subprocess.Popen([sys.executable, "mcp_server.py"], cwd=Path(__file__).parent, env=mcp_env)
                self.processes.append(("MCP Server", mcp_proc))
                time.sleep(3)

                if mcp_proc.poll() is not None:
                    logger.error("âŒ MCP Server failed to start!")
                    return False

                logger.info(f"   âœ… MCP Server started (PID: {mcp_proc.pid})")
                logger.info(f"   ğŸŒ MCP Endpoint: http://localhost:{self.mcp_port}/mcp")
                logger.info("")

            # å¯åŠ¨æˆåŠŸ
            logger.info("=" * 70)
            logger.info("âœ… All Services Started Successfully!")
            logger.info("=" * 70)
            
            if self.start_local_vllm:
                logger.info(f"   â€¢ VLLM Service:       http://localhost:{self.vllm_port}/v1")
            
            logger.info(f"   â€¢ API Documentation:  http://localhost:{self.api_port}/docs")
            logger.info("")
            logger.info("âš ï¸  Press Ctrl+C to stop all services")
            
            self.check_ocr_models()
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to start services: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self.stop_services()
            return False

    def stop_services(self, signum=None, frame=None):
        """åœæ­¢æ‰€æœ‰æœåŠ¡"""
        logger.info("")
        logger.info("=" * 70)
        logger.info("â¹ï¸  Stopping All Services...")
        logger.info("=" * 70)

        # å€’åºå…³é—­ï¼Œå…ˆå…³åå¯åŠ¨çš„ï¼Œæœ€åå…³æœ€åŸºç¡€çš„æœåŠ¡
        for name, proc in reversed(self.processes):
            if proc.poll() is None:
                logger.info(f"   Stopping {name} (PID: {proc.pid})...")
                proc.terminate()

        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        for name, proc in reversed(self.processes):
            try:
                proc.wait(timeout=10)
                logger.info(f"   âœ… {name} stopped")
            except subprocess.TimeoutExpired:
                logger.warning(f"   âš ï¸  {name} did not stop gracefully, forcing...")
                proc.kill()
                proc.wait()

        logger.info("=" * 70)
        logger.info("âœ… All Services Stopped")
        logger.info("=" * 70)
        sys.exit(0)

    def wait(self):
        """ç­‰å¾…æ‰€æœ‰æœåŠ¡"""
        try:
            while True:
                time.sleep(1)
                for name, proc in self.processes:
                    if proc.poll() is not None:
                        logger.error(f"âŒ {name} unexpectedly stopped!")
                        self.stop_services()
                        return
        except KeyboardInterrupt:
            self.stop_services()


def main():
    """ä¸»å‡½æ•°"""
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
    
    parser = argparse.ArgumentParser(
        description="MinerU Tianshu - ç»Ÿä¸€å¯åŠ¨è„šæœ¬ (æ”¯æŒ VLLM)",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # åŸºç¡€é…ç½®
    parser.add_argument("--output-dir", type=str, default="/tmp/mineru_tianshu_output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--api-port", type=int, default=8000, help="APIç«¯å£")
    parser.add_argument("--worker-port", type=int, default=8001, help="Workerç«¯å£")
    
    # ç¡¬ä»¶é…ç½®
    parser.add_argument("--accelerator", type=str, default="auto", choices=["auto", "cuda", "cpu"])
    parser.add_argument("--workers-per-device", type=int, default=1)
    parser.add_argument("--devices", type=str, default="auto")
    
    # MCP é…ç½®
    parser.add_argument("--enable-mcp", action="store_true", help="å¯ç”¨ MCP Server")
    parser.add_argument("--mcp-port", type=int, default=8002)
    
    # PaddleOCR VLLM ç°æœ‰é…ç½®
    parser.add_argument("--paddleocr-vl-vllm-engine-enabled", action="store_true", default=False, help="å¯ç”¨ PaddleOCR VLLM å¼•æ“é€»è¾‘")
    parser.add_argument("--paddleocr-vl-vllm-api-list", type=parse_list_arg, default=[], help="å¤–éƒ¨ VLLM API åˆ—è¡¨")

    # [æ–°å¢] æœ¬åœ°å¯åŠ¨ VLLM é…ç½®
    parser.add_argument("--start-local-vllm", action="store_true", help="æ˜¯å¦åœ¨æœ¬åœ°å¯åŠ¨ VLLM æœåŠ¡")
    parser.add_argument("--vllm-model-path", type=str, default=None, help="PaddleOCR-VL æ¨¡å‹è·¯å¾„ (å½“å¯ç”¨ local-vllm æ—¶å¿…å¡«)")
    parser.add_argument("--vllm-port", type=int, default=8003, help="æœ¬åœ° VLLM æœåŠ¡ç«¯å£")
    parser.add_argument("--vllm-gpu-util", type=float, default=0.4, help="VLLM æ˜¾å­˜å ç”¨æ¯”ä¾‹ (0.0-1.0)")
    parser.add_argument("--vllm-max-model-len", type=int, default=8192, help="VLLM æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦")

    args = parser.parse_args()

    # å¤„ç† devices
    devices = args.devices
    if devices != "auto":
        try:
            devices = [int(d) for d in devices.split(",")]
        except ValueError:
            devices = "auto"

    # é€»è¾‘æ ¡éªŒï¼šå¦‚æœå¯åŠ¨æœ¬åœ° VLLMï¼Œè‡ªåŠ¨å¼€å¯ engine enable
    if args.start_local_vllm:
        args.paddleocr_vl_vllm_engine_enabled = True
        logger.info("ğŸš€ Local VLLM startup requested, auto-enabling PaddleOCR VLLM Engine.")

    if args.paddleocr_vl_vllm_engine_enabled:
        if not args.paddleocr_vl_vllm_api_list and not args.start_local_vllm:
             logger.error("å¯ç”¨ VLLM å¼•æ“æ—¶ï¼Œå¿…é¡»æä¾› --paddleocr-vl-vllm-api-list æˆ–å¼€å¯ --start-local-vllm")
             sys.exit(1)

    launcher = TianshuLauncher(
        output_dir=args.output_dir,
        api_port=args.api_port,
        worker_port=args.worker_port,
        workers_per_device=args.workers_per_device,
        devices=devices,
        accelerator=args.accelerator,
        enable_mcp=args.enable_mcp,
        mcp_port=args.mcp_port,
        # VLLM å‚æ•°
        paddleocr_vl_vllm_engine_enabled=args.paddleocr_vl_vllm_engine_enabled,
        paddleocr_vl_vllm_api_list=args.paddleocr_vl_vllm_api_list,
        start_local_vllm=args.start_local_vllm,
        vllm_model_path=args.vllm_model_path,
        vllm_port=args.vllm_port,
        vllm_gpu_util=args.vllm_gpu_util,
        vllm_max_model_len=args.vllm_max_model_len
    )

    signal.signal(signal.SIGINT, launcher.stop_services)
    signal.signal(signal.SIGTERM, launcher.stop_services)

    if launcher.start_services():
        launcher.wait()
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()
